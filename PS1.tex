\documentclass[12pt]{article}

\usepackage{answers}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{definition}[2][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{solution}[2][Solution]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Information Theory PS1}%replace with the appropriate homework number
\author{Yuyi Gu\\ %replace with your name
516030910469} %if necessary, replace with your course title
 
\maketitle
%Below is an example of the problem environment
\begin{problem}{1 Maximum entropy}
    \item The normal maximizes the entropy for a given variance. If $Var(x)=a$ is fixed, then prove: $$h(x)\leq \frac{1}{2}\log2\pi ea$$
\end{problem}

%Below is the solution environment
\begin{proof}{}
    \item
    \begin{align*}
        D(p||q) &= E_p(\log\frac{p(x)}{q(x)}) \\
                &= E_p(\log p(x))-E_p(\log{\frac{1}{\sqrt{2\pi a}} exp(-\frac{x^2}{2a}})) \\
                &= E_p(\log p(x))-\log (\frac{1}{\sqrt{2\pi a}})-E_p(exp(-\frac{x^2}{2a})) \\
                &= E_p(\log p(x))+\frac{1}{2}\log2\pi a +E_p(-\frac{x^2}{2a}) \\
                &= E_p(\log p(x)))+\frac{1}{2}\log2\pi a+\frac{a+\mu_p^2}{2a}\geq0 \\
    \end{align*}
    $$
    \therefore
        -E_p(\log p(x))\leq \frac{1}{2}\log2\pi ea+\frac{1}{2}\mu_p^2
    $$
    i.e.:
    $$
        H(p)\leq \frac{1}{2}\log2\pi ea
    $$

\end{proof}

\begin{problem}{2 Entropy of a disjoint mixture}
    \item Let $X_{1}$ and $X_{2}$ be discrete random variables drawn according to probability mass function $p_{1}(.)$ and $p_{2}(.)$ over the respective alphabets $\chi_{1}=\{1,2,...,m\}$ and $\chi_{2}=\{m+1,...,n\}$. Let
    $$X=\left\{
    \begin{array}{rcl}
    X_1 & & {, \  with \ probability \  \alpha.} \\
    X_2 & & {, \ with \ probability \  1-\alpha.} 
    \end{array}
    \right.
    $$
    \begin{enumerate}[label=(\arabic*)]
    \item Find $H(x)$ in terms of $H\left(x_1\right)$, $H\left(x_2\right)$ and $\alpha$.
    \item What if $\chi_{1}= \chi_{2}$ ?
    \end{enumerate}
\end{problem}

\begin{solution}{}
    \begin{enumerate}[label =(\arabic*)]
    \item 
        \begin{equation*}
        H_{1}(x) = \sum_{i=1}^m\frac{1}{m}\log\frac{1}{m}
        \end{equation*}
        \begin{equation*}
        H_{2}(x) = \sum_{i=1}^m\frac{1}{n-m}\log\frac{1}{n-m}
        \end{equation*}
        \begin{align*}
            H(x) &= -[(\sum_{x\in\chi_{1}}p_{1}(x)*\alpha*\log p_{1}(x)) + 
            (\sum_{x\in\chi_{2}}p_{2}(x)*(1-\alpha)*\log p_{2}(x))] \\
                 &= \sum_{i=1}^m\frac{\alpha}{m}\log\frac{1}{m} - 
                 \sum_{i=1}^{n-m}\frac{1 - \alpha}{n-m}\log\frac{1 - \alpha}{n-m} \\
                 &= \alpha H_{1}(x) + (1-\alpha)H_{2}(x) - \alpha\log a - (1-\alpha)log(1-\alpha)
        \end{align*}
    \item
        \begin{equation*}
        p(x) = \frac{1}{m}\alpha + \frac{1}{m}(1-\alpha) = \frac{1}{m}
        \end{equation*}
        \begin{align*}
            H(x) &= -\sum_{x\in\cup{\chi_{1},\chi_{2}}} p(x)\log p(x) \\
                &= -\sum_{i=1}^m\frac{1}{m}\log\frac{1}{m} \\
                &= -\log\frac{1}{m} \\
                &= m H_{1}(x) = m H_{2}(x)
        \end{align*}
    \end{enumerate}
\end{solution}

\begin{problem}{3 Entropy of a sum}
    \item 
        Let $X$ and $Y$ be random variables that take on values $x_{1},x_{2},...,x_{r}$ and $y_1,y_2,...,y_s$, respectively.\\Let $Z=X+Y$
    \begin{enumerate}[label =(\alph*)]
    \item
        Show that $H(Z|X)=H(Y|X)$. Argue that if X,Y are independent, then $H(Y)\leq H(Z)$ and  $H(X)\leq H(Z)$. Thus , the addition of independent random variables adds uncertainty.
    \item
        Give an example of (necessarily dependent) random variables in which $H(X)>H(Z)$ and $H(Y)>H(Z)$
    \item
        Under what conditions does $H(Z)=H(X)+H(Y)$ ?
    \end{enumerate}
\end{problem}

\begin{solution}{}
    \begin{enumerate}[label=\alph*)]
        \item
            \begin{align*}
                H(Z|X) &= -\sum_{X,Z}p(X=x,Z=z)*\log p(Z=z|X=x) \\
                    &= -\sum_{X,Z}p(X=x)P(Z=z|X=x)*\log p(Z=z|X=x) \\
                    &= -\sum_{X}p(X=x)*\sum_{Z}P(Z=z|X=x)*\log p(Z=z|X=x) \\
                    &= -\sum_{X}p(X=x)*\sum_{Y}P(Y=z-x|X=x)*\log p(Y=z-x|X=x) \\
                    &= -\sum_{X,Y}p(Y=z-x,X=x)*\log p(Y=z-x|X=x) \\
                    &= H(Y|X) \\
            \end{align*}
            if X,Y independent \quad $H(Y|X)=H(Y)$
            \begin{align*}
                H(Z) &= H(X+Y) \\
                    &= H(X+Y|Y) + I(X+Y;Y) \\
                    &{\geq} H(X|Y) = H(X) \\
            \end{align*}
            Similarly,$H(Z) \geq H(Y)$
        \item
            W.O.L.G, suppose the joint probability distribution of $X$ and $Y$ is $$p(X=0,Y=0)=\frac{1}{2}, p(X=0,Y=-1) = 0,$$
            $$ p(X=1,Y=0)=0, p(X=1,Y=-1)=\frac{1}{2}$$. \\
            it is easy to verify that the probability distribution of $Z$ is
            $$p(Z=0)=1$$
            Thus,
            $$H(Z)=0$$
            Also,since $p(X=0)=\frac{1}{2}, p(X=1)=\frac{1}{2}$
            $$H(X)=-\left(\frac{1}{2}\log\frac{1}{2}+\frac{1}{2}\log\frac{1}{2}\right) = 0.3 > H(Z)$$
            Similarly,since $p(Y=0)=\frac{1}{2}, p(Y=-1)=\frac{1}{2}$
            $$H(Y)=-\left(\frac{1}{2}\log\frac{1}{2}+\frac{1}{2}\log\frac{1}{2}\right) = 0.3 > H(Z)$$  
        \item
            $$H(X,Y)=H(X)+H(Y|X)\leq H(X)+H(Y)$$
            $$H(Z)\leq H(X,Y) \leq H(X)+H(Y)$$
            \begin{center}
            the equal holds only when $H(Y|X)=H(Y)$,i.e.:\\
            X,Y independent
            \end{center}
    \end{enumerate}
\end{solution}

\begin{problem}{4 Entropy and Pairwise independence}
    \item Let X,Y,Z be three binary $Bernoulli\left(\frac{1}{2}\right)$ random variables that are pairwise independent; that is, 
    $$I(X;Y)=I(X;Z)=I(Y;Z)=0$$
    \begin{enumerate}[label=\alph*)]
        \item 
        Under this constraint, what is the minimum value for H(X,Y,Z)?
        \item
        Give an example achieving this minimum.
    \end{enumerate}    
\end{problem}

\begin{solution}{}
    \begin{enumerate}[label=\alph*)]
        \item 
        By the information given in the question, we can include that
        $$H(X)=H(Y)=H(Z)=-\frac{1}{2}$$
        So, we can draw the following information diagram\\
        \includegraphics[scale=0.6]{PS1.png}\\
        the range of $a$ in the graph is $0\leq a\leq1$\\
        since the mutual information is non-negative\\
        By observation, we can obtain that
        $$H(X,Y,Z)=-3\log_2\frac{1}{2}-a=3-a\geq 2 $$
        \item
            Suppose $Z=(X+Y)mod2$, and the probability distribution of $X$ and $Y$ is as follows:
            \begin{spacing}{1.5}
            $$p(X)=\left\{
            \begin{array}{rcl}
            \frac{1}{2} & & {X=1} \\
            \frac{1}{2} & & {X=0} 
            \end{array}
            \right.
            \quad
            p(Y)=\left\{
            \begin{array}{rcl}
            \frac{1}{2} & & {Y=1} \\
            \frac{1}{2} & & {Y=0} 
            \end{array}
            \right.
            $$
            \end{spacing}
            Then, it is easy to figure out 
            $$I(Z;X|Y)=H(Z|Y)-H(Z|X,Y)=-\log_2\frac{1}{2}=1$$
            $$H(X,Y,Z)=2$$
    \end{enumerate}
\end{solution}

\begin{problem}{5 Subset inequality}
    \item
    Prove that 
    $$\frac{1}{2}[H(X_1,X_2)+H(X_2,X_3)+H(X_3,X_1)]\geq H(X_1,X_2,X_3)$$
\end{problem}

\begin{proof}{}
    \item
    \begin{align*}
        left &= \frac{1}{2}[H(X_1,X_2)+H(X_2,X_3)+H(X_3,X_1)] \\
            &= \frac{1}{2}[2H(X_1|X_3)+I(X_1;X_3)+2H(X_3)+H(X_2|X_1)+H(X_2|X_3)] \\
            &= \frac{1}{2}[2H(X_1|X_3)+2H(X_3)+2H(X_2)-I(X_1;X_3)-I(X_1;X_2)+I(X_1;X_3)] \\
            &= \frac{1}{2}[2H(X_1|X_3)+2H(X_3)+2H(X2|X1,X3)]+\frac{1}{2}[I(X_1;X_3)+I(X_1;X_2)+I(X_2;X_3)] \\
            &= H(X,Y,Z)+\frac{1}{2}[I(X1;X3)+I(X2;X3)+I(X1;X3)]
    \end{align*}
    $$\because
        I(X_1;X_2),I(X_2;X_3),I(X_1;X_3)\geq 0
    $$
    $$
    \therefore
        \frac{1}{2}[H(X_1,X_2)+H(X_2,X_3)+H(X_3,X_1)]\geq H(X_1,X_2,X_3)
    $$
\end{proof}

\begin{problem}{6 Information diagram for Markov Chain}
    \item
    Verify that if $X->Y->Z$, then their information diagram can be simplified as 
    \newline
    \includegraphics[scale=0.5]{PS1-2.png}\\
    (The result can be extended to $x_1->x_2->...->x_n$)
\end{problem}
\begin{proof}{}
    \item
    \begin{center}
    prove the information diagram $\Longleftrightarrow$ prove $I(X;Z\big|Y)=0$ \\
    \end{center}
    \begin{align*}
        I(X;Z|Y) &= H(X|Y)+H(Z|Y)-H(X,Z|Y)\\
                &= H(X|Y)+H(Z|Y)-(H(X|Z,Y)+H(Z|X,Y))\\
                &= H(X|Y)+H(Z|Y)-H(X|Y)-H(Z|Y)\\
                &= 0
    \end{align*}
\end{proof}

\begin{problem}{7 Implication and Markov Chain}
    \quad\quad\quad
    \begin{enumerate}[label=\alph*)]
        \item 
        Prove that under the constraint that $X->Y->Z$ forms a Markov chain,  $X\perp Y|Z$ and $X\perp Z$ imply $X\perp Y$.
        \item
        Prove that the implication in (a) continues to be valid without the Markov chain constraint.
        \item
        Prove that $Y\perp Z|T$ implies $Y\perp Z|(X,T)$ conditioning on $X->Y->Z->T$
    \end{enumerate}
\end{problem}

\begin{proof}
    \begin{enumerate}[label=\alph*]
        \item
        $$X\perp Z \Longleftrightarrow I(X;Z)=0$$
        \begin{center}
        So, it is easy to draw the information diagram conditioning on the Markov chain. \\
        \includegraphics[scale=0.5]{PS1-3.png}\\
        \end{center}
        Also,
        $$x\perp Y\big| Z \Longleftrightarrow I(X;Y|Z)=0$$
        Conditioning on what have been discussed above
        $$I(X;Y|Z)=0\Longleftrightarrow I(X;Y)=0$$
        $$\therefore X\perp Y$$
        \item 
        $$\because I(X;Y\big|Z)=0, I(X;Z)=0$$
        W.L.O.G. suppose $I(X;Z|Y)=a$, since the mutual information entropy is non-negative, $a\geq0$
        $$\because I(X;Z)=0 $$
        $\therefore $
        we can figure out the information diagram as follows:
        \begin{center}
            \includegraphics[scale=0.5]{PS1-4.png}
        \end{center}
        According to the graph, we can conclude $I(X;Y)=-a$. Moreover, since the mutual information is non-negative, $$-a\geq0$$
        since $a\geq0$, so we can derive
        $$-a\leq0$$
        $$\therefore a=0$$
        $$\Longleftrightarrow I(X;Y)=-a=0$$
        $$\Longleftrightarrow X\perp Y$$
        \item
        $$Y\perp Z\big| T \Leftrightarrow I(Y;Z|T)=0$$
        $$\Leftrightarrow H(X|T)+H(Z|T)-H(X,Z|T)=0$$
        $$\Leftrightarrow H(X)+H(Z)=H(X,Z|T)$$
        $$\Leftrightarrow H(X)+H(Z)=H(X|Z,T)+H(Z|X,T)$$
        $$\Leftrightarrow H(Z)=H(Z|X)$$
        $$\Leftrightarrow I(X;Z)=0$$
        $$\Leftrightarrow I(Y;Z|T)=I(Y;Z|X,T)$$
    \end{enumerate}
\end{proof}

\begin{problem}{8 Markov chain with 4 random variables}
    \item
    Let $X->Y->Z->T$ form a Markov chain. Determine which of the following inequalities always hold:
    \begin{enumerate}[label=\roman*]
        \item $$I(X;T)+I(Y;Z)\geq I(X;Z)+I(Y;T)$$
        \item $$I(X;T)+I(Y;Z)\geq I(X;Y)+I(Z;T)$$
        \item $$I(X;Y)+I(Z;T)\geq I(X;Z)+I(Y;T)$$
    \end{enumerate}
\end{problem}

\begin{solution}{}
    \item
    Inequalities i and iii holds.\\
    As the picture below shows,
    \begin{center}
        \includegraphics[scale=0.5]{PS1-5.png}
    \end{center}
    \begin{enumerate}[label=\roman*]
        \item
        \begin{align*}
            I(X;Z)-I(X;T)&=[H(X)-H(X|Z)]-[H(X)-H(X|T)]\\
                        &= H(X|T)-H(X|Z)\\
                        &= I(Z;X|T)
        \end{align*}
        Similarly, $$I(Y;Z)-I(Y;T)=I(Y;Z|T)$$
        \begin{equation}  
        \left\{  
             \begin{array}{lr}  
             I(X;T)=I(X;Z)-I(Z;X|T) &  \\  
             I(Y;Z)=I(Y;T)+I(Y;Z|T) &  \\  
             \end{array}  
        \right.  
        \end{equation}  
        \begin{center}
            \includegraphics[scale=0.5]{PS1-7.png}\\
            The red part denotes: $I(Z;X|T)$
            The blue part denotes: $I(Y;Z|T)$
        \end{center}
        $$\because I(Z;X|T)\leq I(Y;Z|T)$$
        $\therefore$ 
        \begin{equation}
            I(X;T)+I(Y;Z)\geq I(X;Z)+I(Y;T)
        \end{equation}
        
        \item
        Just like discussed above,
        $$I(X;Y)-I(X;T)=I(X;Y|T)$$
        \begin{align*}
            I(Y;Z)-I(Z;T) &= [H(Z)-H(Z|Y)]-[H(Z)-H(Z|T)]\\
                          &= H(Z|T)-H(Z|Y)\\
        \end{align*}
        $\therefore$ We can't determine the sign of the inequality\\
        \item
        Similarly,we can derive
        \begin{equation}  
        \left\{  
             \begin{array}{lr}  
             I(X;Y)-I(X;Z)=I(X;Y|Z) &  \\  
             I(Z;T)-I(Y;T)=I(Z;T|Y) &  \\  
             \end{array}  
        \right.  
        \end{equation} 
        $\longrightarrow$
        \begin{equation}
            I(X;Y)+I(Z;T)\geq I(X;Z)+I(Y;T)
        \end{equation}
    \end{enumerate}
\end{solution}
\newpage
\begin{problem}{9 Imperfect secrecy theorem}
    \item
    Let X be the plain text, Y be the cipher text, and Z be the key in a secret key cryptosystem. Since X can be recovered from Y and Z, we have
    $$H(X|Y,Z)=0$$
    We will show that this constraint implies 
    $$I(X;Y)\geq H(X)-H(Z)$$
\end{problem}

\begin{proof}
    \item
    By drawing the information graph, we can easily come to the following equation:
    \begin{equation}
        H(Z)= H(Z|X,Y)+I(Y;Z)+I(X;Z|Y)
    \end{equation}
    \begin{center}
        \includegraphics[scale=0.5]{PS1-6.png}\\
        the purple part denotes $I(Y;Z)\geq0$\\
        the orange part denotes $H(Z|X,Y)\geq0$\\
        the blue part denotes $H(X|Y,Z)=0$\\
        the green part denotes $I(X;Z|Y)$
    \end{center}
    $$\because H(X|Y,Z)=0$$
    $$\therefore I(X;Z|Y)=I(X;Z|Y)+H(X|Y,Z)=H(X|Y)$$
    $$\because H(Z)= H(Z|X,Y)+I(Y;Z)+I(X;Z|Y)$$
    \begin{align*}
    \therefore
        H(Z) &\geq I(X;Z|Y) \\
            &\geq H(X|Y) \\
            &\geq H(X)-I(X;Y)
    \end{align*}
    $$\therefore I(X;Y)\geq H(X)-H(Z)$$
\end{proof}{}

\pagebreak
\end{document}
